Okay, so this, I'm walking, and I'm trying to figure out what to do for this project specifically. It's the ramble project. The idea is, as I walk, I like to walk while thinking to myself about how to work on some things. Actually, no, it's more like the other way around. So I. This is for someone who likes to think think while walking when they can't really write things down, but they also want to keep track of what they're thinking. For me personally, I like to think while walking and then kind of talk to myself, talk through all the different steps in project and just kind of brainstorm talking to myself. Sometimes I record myself to make sure that I get things, but also I don't really have the time to listen to all that recording because then becomes really slow. There are a lot of pauses, et cetera. So this project is, I think the product is going to be an app that has a record button. As you walk, you can just kind of like record whatever, record your thoughts in audio format, and then it summarizes it and puts it in markdown so that you can put it in things like notion or whatever text editor you want, and also so you can review it visually as opposed to having to listen to the whole thing. Okay, so to plan this, I want to plan it. I want to know what the next steps are. Most likely. I think step one is the prototype. I'm recording this actually, as an example of the kind of audio that I'll be getting. Phase one, I'll have to upload the audio and then use assembly AI's API to convert it to text, and then use GPT to convert that text into something meaningful, like a summary in markdown. So step one, figure out the AssemblyAI API, which I've already done. Step two is to pass the transcript into OpenAI, see if there are problems with context length, see what prompt works best with and without context. Once phase one is done, phase two is to actually build the UI. And when we have the UI, I want it to be so that there's a record button, which then records what I say. And then it records what I say. And then when I press stop, it will transcribe it, send it to OpenAI, return the markdown, then I can review the markdown, export it via email. Now, phase three. Phase three is either for. I think phase three will be integrations. If I can integrate it with notion directly, that would be really cool. So it's either that or I will add support for live transcription. I don't think live transcription will be interesting. Sorry, actually, scratch that. I do think it will be interesting. However, I think that live transcriptions aren't what you call it. It's not a major quality of life boost for the user. Therefore, I don't think it needs to be prioritized as much. Because, for example, for me, as I'm walking, I'm walking. I'm not even going to read this until way after. So it actually doesn't matter if it's live or not, because I'm not going to look at the transcripts live anyway. It's more important that the quality is good than for me to get my live transcript, because I won't use it anyway. But it is a cool feature, though. So. Yeah, that may be in phase three or phase four. Phase three could be for. Phase three is probably the notion integration. And then phase four is. Phase four will be this live transcription. So, to summarize, phase one. Phase one will be to figure out the technical part of it. Sorry, like the science part of it, I guess. The transcription and the summarization using DPT. So that's phase one. Phase two, building the UI with a record button. So instead of having to manually point to a file, I'll just have a button that records it and then connect that recording to the transcription and summarization service. And phase three, better integrations with the likes of notion and maybe other notes. Apps. And phase four enable live transcription. Because I think it's just kind of cool. Although, honestly, it might be kind of useless. What else? Yeah, I think after that, I'll send it over. I'll send it to some people, see what they think. Yeah, that's it. I guess it's.